import numpy as np #numpy를 np로 import

X = np.array([1, 2, 3]) #입력값 x 지정
Y = np.array([1, 2, 3]) #출력값 y 지정

def cost_func(W, X, Y): #데이터 x,y에 대해서 w값이 주어졌을 때 cost를 계산하는 함수
    c = 0 #c는 0으로 지정
    for i in range(len(X)):
        c += (W * X[i] - Y[i]) ** 2 #예측값 - 실제값(=오차)를 제곱한 값
    return c / len(X) #c에 누적덧셈이 되며 편차 제곱을 데이터 갯수로 나눈다(=평균)

for feed_W in np.linspace(-3, 5, num=15): #-3 ~ 5사이를 15개의 구간으로 분할
    curr_cost = cost_func(feed_W, X, Y) #feed_W는 15개 구간값 지님
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost)) #feed_W에 따라 코스트의 값 출력

#TensorFlow   
    X = np.array([1, 2, 3]) #입력값 x 지정
    Y = np.array([1, 2, 3]) #출력값 y 지정

def cost_func(W, X, Y): #데이터 x,y에 대해서 w값이 주어졌을 때 cost를 계산하는 함수
  hypothesis = X * W #가설
  return tf.reduce_mean(tf.square(hypothesis - Y)) #(예측값 - Y)의 제곱값으로 평균 구함

W_values = np.linspace(-3, 5, num=15) #-3 ~ 5사이를 15개의 구간으로 분할후 W_values에 저장
cost_values = [] #list로 값을 받음

for feed_W in W_values: #list값을 각각 추출하여 W_values에 저장
    curr_cost = cost_func(feed_W, X, Y) #cost가 W_values에따라 어떻게 변하는지 기록
    cost_values.append(curr_cost) #변화 값 list에 추가
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost)) #feed_W에 따라 코스트의 값 출력
    
# -3.000 |   74.66667
  -2.429 |   54.85714
  -1.857 |   38.09524
  -1.286 |   24.38095
  -0.714 |   13.71429
  -0.143 |    6.09524
   0.429 |    1.52381
   1.000 |    0.00000
   1.571 |    1.52381
   2.143 |    6.09524
   2.714 |   13.71429
   3.286 |   24.38095
   3.857 |   38.09524
   4.429 |   54.85714
   5.000 |   74.66667
   
# Gradient descent 

 tf.set_random_seed(0) #재구현해도 동일하도록 random_seed를 특정한 값으로 초기화
 x_data = [1.,2.,3.,4.] #x_data(=입력값) 지정
 y_data = [1.,3.,5.,7.] #y_data(=출력값) 지정
 w=tf.Variable(tf.random_normal([1],-100.,100.)) #정규분포의 랜덤변수를 1개짜리로 변수로 지정 후 w에 저장
 for step in range(300): #300번을 반복한다
    hypothesis = W * X # hypothesis 정의
    cost = tf.reduce_mean(tf.square(hypothesis - Y)) #cost 정의

    alpha = 0.01 # alpha값을 0.01로 정의
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X)-Y,X) #gradient(=기울기) = X의 평균을 구한것 
    descent= W - tf.multiply(alpha,gradient) # W - gradient를 구한 후 alpha값을 곱한 값 = 새로운 W 값                                              
    W.assign(descent) # 새로운 W 값을 W에 할당하며 업데이트 한다
    
    if step % 10 == 0: # 10번에 한번씩 cost값과 W값 출력
        print('{:5}|{:10.4f}|{:10.6f}'.format(step,cost.numpy(),w.numpy()[0]))
                       
    
#  step   cost        w  
    0|11716.3086| 48.767971 # cost값이 (큰값 -> 0값)수렴, W값이 (랜덤 값 -> 1.0)수렴
   10| 4504.9126| 30.619968 # W값에 관계 없이 코스트->0, W->특정한 값으로 수렴
   20| 1732.1364| 19.366755                   
   30|  666.0052| 12.388859                    
   50|   98.4620|  5.379007                    
   60|   37.8586|  3.715335
   70|   14.5566|  2.683725
   80|    5.5970|  2.044044
   90|    2.1520|  1.647391
  100|    0.8275|  1.401434
  110|    0.3182|  1.248922
  120|    0.1223|  1.154351
  130|    0.0470|  1.095710
  140|    0.0181|  1.059348
  150|    0.0070|  1.036801
  160|    0.0027|  1.022819
  170|    0.0010|  1.014150
  180|    0.0004|  1.008774
  190|    0.0002|  1.005441
  200|    0.0001|  1.003374
  210|    0.0000|  1.002092
  220|    0.0000|  1.001297
  230|    0.0000|  1.000804
  240|    0.0000|  1.000499
  250|    0.0000|  1.000309
  260|    0.0000|  1.000192
  270|    0.0000|  1.000119
  280|    0.0000|  1.000074
  290|    0.0000|  1.000046   
  
